---
layout: post
title: HW4 Reflections on software failures
---

well hello, todays readings all revolved around the failure of software projects and the context in which they were allowed to fail so miserably. I'll talk later about how this related to a previous post, HW2, and the ethics that come with designing software.

First, I think it's important to mention the textbook chapters we had to read for this post. Chapter 13, Security Engineering and Chapter 14, Resilience Engineering go together like peanut butter and jelly. At its core engineering is the study and execution of problem solving skills. No wonder it is our greatest concern to think of time complexity in terms of worst case performance. We as a group of people refuse to accept the negative effects of the technology we created and that is where chapter 13 & 14 come in. For security engineering the focus is much more on protecting the project from "bad actors" as opposed to the negligence of the programmer. Security threats can be threats to the confidentiality, integrity, or availability of a system or its data. In the modern world of Denial of Service attacks the real question becomes is the code resilient enough to be safe. In the case of Cars being connected to the internet it doesn't matter if the code itself has errors if it can be forced to shut down through lack of security. 

The Therac-25 accidents in my eyes acts as the shining example of what goes wrong when code isn't developed ethically/ correctly. For clarification, the Therac-25 incident took place in the 1980's where the software was designed administer radiation to patients. Essentially the developers and management team responsible for this code did not make sure the software was safe to use. This resulted in at least six overdoses of radiation that resulted in deaths and serious injuries. With such a clear example of what not to do, it was very easy for the public to be outraged. As a result the Threac-25 accidents are still taught in classes like ours to this day. 

